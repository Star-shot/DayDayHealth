"""
LLM ç›¸å…³å·¥å…·å‡½æ•°
"""

import pandas as pd
import yaml
import base64
from pathlib import Path
from openai import OpenAI
from typing import Generator


def encode_image_to_base64(image_path: str) -> str:
    """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def get_image_mime_type(image_path: str) -> str:
    """è·å–å›¾ç‰‡MIMEç±»å‹"""
    suffix = Path(image_path).suffix.lower()
    mime_types = {
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
        ".gif": "image/gif",
        ".webp": "image/webp"
    }
    return mime_types.get(suffix, "image/jpeg")


def load_data(filepath):
    """
    åŠ è½½æ•°æ®é›†ï¼Œæ”¯æŒ xlsx å’Œ csv æ–‡ä»¶ï¼Œåˆ é™¤ç¼ºå¤±å€¼ï¼Œé»˜è®¤æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾ã€‚
    """
    if filepath.endswith('.xlsx'):
        df = pd.read_excel(filepath)
    elif filepath.endswith('.csv'):
        df = pd.read_csv(filepath)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒ .xlsx å’Œ .csv æ–‡ä»¶ã€‚")
    
    df = df.dropna()
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values
    
    return X, y


def load_config():
    """åŠ è½½å®Œæ•´é…ç½®"""
    config_path = Path(__file__).parent.parent.parent / "config.yaml"
    if not config_path.exists():
        # å°è¯•ç›¸å¯¹è·¯å¾„
        config_path = Path("../config.yaml")
    if not config_path.exists():
        raise FileNotFoundError("æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ config.yaml")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def get_llm_config(agent_id: str = None):
    """
    æ ¹æ®æ™ºèƒ½ä½“è·å–å¯¹åº”çš„ LLM é…ç½®
    è¿”å›: (api_key, base_url, model_name)
    """
    config = load_config()
    
    if agent_id and agent_id in config.get('agent_models', {}):
        agent_config = config['agent_models'][agent_id]
        provider_name = agent_config.get('provider', config.get('default_provider', 'kimi'))
        model_type = agent_config.get('model', 'default')
    else:
        provider_name = config.get('default_provider', 'kimi')
        model_type = 'default'
    
    providers = config.get('llm_providers', {})
    if provider_name not in providers:
        raise ValueError(f"æœªæ‰¾åˆ°æä¾›å•†é…ç½®: {provider_name}")
    
    provider = providers[provider_name]
    api_key = provider['api_key']
    base_url = provider['base_url']
    
    models = provider.get('models', {})
    model_name = models.get(model_type, models.get('default', 'gpt-3.5-turbo'))
    
    return api_key, base_url, model_name


def validate_messages(messages: list) -> list:
    """ç¡®ä¿æ¶ˆæ¯è§’è‰²ä¸¥æ ¼äº¤æ›¿ï¼Œåˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯"""
    if not messages:
        return []
    
    validated = []
    i = 0
    while i < len(messages):
        msg = messages[i]
        role = msg["role"]
        content = msg["content"]
        
        j = i + 1
        while j < len(messages) and messages[j]["role"] == role:
            next_content = messages[j]["content"]
            if isinstance(content, list) and isinstance(next_content, list):
                content = content + next_content
            elif isinstance(content, list):
                content = content + [{"type": "text", "text": str(next_content)}]
            elif isinstance(next_content, list):
                content = [{"type": "text", "text": str(content)}] + next_content
            else:
                content = f"{content}\n{next_content}"
            j += 1
        
        validated.append({"role": role, "content": content})
        i = j
    
    return validated


def get_system_prompt(model_id: str) -> str:
    """æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è·å–ç³»ç»Ÿæç¤ºè¯"""
    prompts = {
        'ç–¾ç—…è¯Šæ–­': """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºç–¾ç—…è¯Šæ–­è¾…åŠ©ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
- æ ¹æ®ç”¨æˆ·æè¿°çš„ç—‡çŠ¶ï¼Œåˆ†æå¯èƒ½çš„ç–¾ç—…ç±»å‹
- è§£è¯»åŒ»å­¦æ£€æŸ¥æŠ¥å‘Šå’Œå½±åƒèµ„æ–™
- æä¾›åˆæ­¥çš„è¯Šæ–­å»ºè®®å’Œå°±åŒ»æŒ‡å¯¼
- è¯´æ˜ç–¾ç—…çš„ç—…å› ã€ç—‡çŠ¶ç‰¹å¾å’Œå‘å±•è¶‹åŠ¿

é‡è¦æé†’ï¼š
1. ä½ æä¾›çš„æ˜¯è¾…åŠ©å‚è€ƒæ„è§ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­
2. é‡åˆ°ç´§æ€¥æˆ–ä¸¥é‡ç—‡çŠ¶ï¼Œè¯·å»ºè®®ç”¨æˆ·ç«‹å³å°±åŒ»
3. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€é€šä¿—æ˜“æ‡‚
4. ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œä¸è¯¢é—®ä¸å¿…è¦çš„ä¸ªäººä¿¡æ¯""",

        'å¥åº·ç®¡ç†': """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¥åº·ç®¡ç†AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºä¸ªäººå¥åº·æŒ‡å¯¼ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
- åˆ†æç”¨æˆ·çš„å¥åº·æ•°æ®å’Œç”Ÿæ´»ä¹ æƒ¯
- æä¾›ä¸ªæ€§åŒ–çš„å¥åº·æ”¹å–„å»ºè®®
- åˆ¶å®šç§‘å­¦çš„è¿åŠ¨è®¡åˆ’å’Œä½œæ¯å®‰æ’
- è§£ç­”æ…¢æ€§ç—…ç®¡ç†å’Œé¢„é˜²ä¿å¥é—®é¢˜
- æä¾›å¿ƒç†å¥åº·å’Œå‹åŠ›ç®¡ç†å»ºè®®

é‡è¦æé†’ï¼š
1. å»ºè®®è¦ç§‘å­¦ã€å®ç”¨ã€å¾ªåºæ¸è¿›
2. è€ƒè™‘ç”¨æˆ·çš„å®é™…æƒ…å†µï¼Œç»™å‡ºå¯æ‰§è¡Œçš„æ–¹æ¡ˆ
3. é‡åˆ°éœ€è¦åŒ»ç–—å¹²é¢„çš„æƒ…å†µï¼ŒåŠæ—¶å»ºè®®å°±åŒ»
4. é¼“åŠ±å¥åº·ç”Ÿæ´»æ–¹å¼ï¼Œä½†ä¸è¦è¿‡åº¦ç„¦è™‘""",

        'è¥å…»æŒ‡å¯¼': """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¥å…»å’¨è¯¢AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºè†³é£Ÿè¥å…»æŒ‡å¯¼ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
- åˆ†æç”¨æˆ·çš„é¥®é£Ÿç»“æ„å’Œè¥å…»çŠ¶å†µ
- æä¾›ä¸ªæ€§åŒ–çš„è†³é£Ÿæ­é…å»ºè®®
- è§£ç­”é£Ÿç‰©è¥å…»ã€é¥®é£Ÿç¦å¿Œç­‰é—®é¢˜
- é’ˆå¯¹ç‰¹å®šäººç¾¤ï¼ˆå­•å¦‡ã€è€äººã€å„¿ç«¥ç­‰ï¼‰æä¾›è¥å…»æ–¹æ¡ˆ
- å¸®åŠ©ç®¡ç†ä½“é‡ã€æ”¹å–„äºšå¥åº·çŠ¶æ€

é‡è¦æé†’ï¼š
1. å»ºè®®è¦ç§‘å­¦åˆç†ï¼Œç¬¦åˆè¥å…»å­¦åŸåˆ™
2. è€ƒè™‘ç”¨æˆ·çš„å£å‘³åå¥½å’Œå®é™…æ¡ä»¶
3. ç‰¹æ®Šç–¾ç—…æ‚£è€…çš„é¥®é£Ÿå»ºè®®éœ€è°¨æ…
4. ä¸æ¨èæç«¯èŠ‚é£Ÿæˆ–ä¸å¥åº·çš„å‡è‚¥æ–¹æ³•"""
    }
    return prompts.get(model_id, prompts['å¥åº·ç®¡ç†'])


def parse_response_content(content) -> str:
    """è§£ææ¨¡å‹è¿”å›çš„å†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
    if isinstance(content, str):
        return content
    
    if isinstance(content, list):
        texts = []
        for item in content:
            if isinstance(item, dict):
                if item.get('type') == 'text' and 'text' in item:
                    texts.append(item['text'])
                elif 'content' in item:
                    texts.append(str(item['content']))
            else:
                texts.append(str(item))
        return ''.join(texts)
    
    return str(content) if content else ""


def stream_response(messages: list, model_id: str) -> Generator[str, None, None]:
    """æµå¼å“åº”ç”Ÿæˆå™¨"""
    full_response = ""
    api_key, base_url, model_name = get_llm_config(model_id)
    
    client = OpenAI(
        api_key=api_key,
        base_url=base_url
    )
    
    try:
        system_prompt = get_system_prompt(model_id)
        system_msg = [{"role": "system", "content": system_prompt}]
        validated_msgs = system_msg + validate_messages(messages)
        
        stream = client.chat.completions.create(
            model=model_name,
            messages=validated_msgs,
            stream=True,
            temperature=0.7,
        )
        for chunk in stream:
            delta_content = chunk.choices[0].delta.content
            if delta_content:
                parsed = parse_response_content(delta_content)
                full_response += parsed
                yield full_response
    except Exception as e:
        yield f"APIé”™è¯¯: {str(e)}"


def chat(history: list, model_id: str, image_cache: str = None) -> Generator[list, None, None]:
    """å¤„ç†èŠå¤©äº¤äº’"""
    messages = []
    
    for i, entry in enumerate(history):
        if isinstance(entry, dict):
            content = entry.get("content")
            role = entry.get("role")
            if content and role:
                if isinstance(content, list):
                    text_parts = [item.get("text", str(item)) if isinstance(item, dict) else str(item) for item in content]
                    content_str = " ".join(text_parts)
                else:
                    content_str = str(content)
                
                is_last_user_msg = (i == len(history) - 1 and role == "user" and image_cache)
                
                if is_last_user_msg:
                    text = content_str.replace("ğŸ“· [å·²ä¸Šä¼ å›¾ç‰‡]\n", "")
                    base64_image = encode_image_to_base64(image_cache)
                    mime_type = get_image_mime_type(image_cache)
                    api_content = [
                        {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}},
                        {"type": "text", "text": text}
                    ]
                else:
                    api_content = content_str
                
                messages.append({"role": role, "content": api_content})
        else:
            if entry[0]: messages.append({"role": "user", "content": str(entry[0])})
            if entry[1]: messages.append({"role": "assistant", "content": str(entry[1])})
    
    response_generator = stream_response(messages, model_id)
    
    try:
        for partial_response in response_generator:
            yield history + [{"role": "assistant", "content": partial_response}]
    except Exception as e:
        yield history + [{"role": "assistant", "content": f"é”™è¯¯: {str(e)}"}]

