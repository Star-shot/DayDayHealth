"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
åŒ…å«ï¼šç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼å¤„ç†ã€æ•°æ®åˆ†å¸ƒæ ¡æ­£ã€ç›¸å…³æ€§åˆ†æ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.impute import KNNImputer
from typing import Tuple, List, Optional, Dict
import warnings
import matplotlib
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
def setup_matplotlib_chinese():
    """é…ç½® matplotlib ä¸­æ–‡å­—ä½“"""
    import subprocess
    
    # ä¼˜å…ˆä½¿ç”¨çš„ä¸­æ–‡å­—ä½“åˆ—è¡¨ï¼ˆæŒ‰å¯é æ€§æ’åºï¼‰
    preferred_fonts = [
        'WenQuanYi Micro Hei',
        'WenQuanYi Zen Hei', 
        'Noto Sans CJK SC',
        'Source Han Sans SC',
        'Droid Sans Fallback',
        'SimHei',
        'Microsoft YaHei',
    ]
    
    plt.rcParams['axes.unicode_minus'] = False
    
    # è·å– matplotlib å®é™…å¯ç”¨çš„å­—ä½“
    try:
        from matplotlib.font_manager import fontManager, findfont, FontProperties
        available = {f.name for f in fontManager.ttflist}
        
        for font in preferred_fonts:
            if font in available:
                # éªŒè¯å­—ä½“ç¡®å®å¯ç”¨ï¼ˆä¸ä¼š fallbackï¼‰
                try:
                    fp = FontProperties(family=font)
                    font_path = findfont(fp, fallback_to_default=False)
                    if font_path and 'DejaVu' not in font_path:
                        plt.rcParams['font.sans-serif'] = [font, 'DejaVu Sans']
                        return font
                except:
                    continue
    except:
        pass
    
    # æ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤è‹±æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    return None

# åˆå§‹åŒ–å­—ä½“ï¼ˆsetup å‡½æ•°å·²ç»éªŒè¯è¿‡å­—ä½“å¯ç”¨æ€§ï¼‰
_CHINESE_FONT = setup_matplotlib_chinese()
_HAS_CHINESE = _CHINESE_FONT is not None

warnings.filterwarnings('ignore')

# æœ€å¤§æ˜¾ç¤ºç‰¹å¾æ•°
MAX_DISPLAY_FEATURES = 8


def get_display_text(chinese: str, english: str) -> str:
    """æ ¹æ®å­—ä½“æ”¯æŒè¿”å›ä¸­æ–‡æˆ–è‹±æ–‡æ–‡æœ¬"""
    if _HAS_CHINESE:
        return chinese
    return english


class DataProcessor:
    """æ•°æ®é¢„å¤„ç†ç±»"""
    
    def __init__(self, df: pd.DataFrame):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            df: è¾“å…¥çš„ DataFrame
        """
        self.df = df.copy()
        self.original_df = df.copy()
        self.processing_log = []
    
    def get_data(self) -> pd.DataFrame:
        """è·å–å¤„ç†åçš„æ•°æ®"""
        return self.df
    
    def reset(self):
        """é‡ç½®ä¸ºåŸå§‹æ•°æ®"""
        self.df = self.original_df.copy()
        self.processing_log = []
    
    def _select_representative_features(self, columns: List[str] = None, 
                                          max_features: int = MAX_DISPLAY_FEATURES) -> List[str]:
        """
        é€‰æ‹©ä»£è¡¨æ€§ç‰¹å¾ï¼ˆåŸºäºæ–¹å·®å’Œç›¸å…³æ€§ï¼‰
        
        Args:
            columns: å€™é€‰åˆ—
            max_features: æœ€å¤§ç‰¹å¾æ•°
            
        Returns:
            é€‰ä¸­çš„ç‰¹å¾åˆ—è¡¨
        """
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(columns) <= max_features:
            return columns
        
        # è®¡ç®—æ¯åˆ—çš„æ–¹å·®ï¼ˆæ ‡å‡†åŒ–åï¼‰
        df_numeric = self.df[columns].dropna()
        if len(df_numeric) == 0:
            return columns[:max_features]
        
        # æ ‡å‡†åŒ–åè®¡ç®—æ–¹å·®
        df_scaled = (df_numeric - df_numeric.mean()) / (df_numeric.std() + 1e-8)
        variances = df_scaled.var().sort_values(ascending=False)
        
        # é€‰æ‹©æ–¹å·®æœ€å¤§çš„ç‰¹å¾ï¼ŒåŒæ—¶é¿å…é«˜ç›¸å…³æ€§
        selected = []
        corr_matrix = df_numeric.corr().abs()
        
        for col in variances.index:
            if len(selected) >= max_features:
                break
            
            # æ£€æŸ¥ä¸å·²é€‰ç‰¹å¾çš„ç›¸å…³æ€§
            is_redundant = False
            for s in selected:
                if corr_matrix.loc[col, s] > 0.85:  # ç›¸å…³ç³»æ•°é˜ˆå€¼
                    is_redundant = True
                    break
            
            if not is_redundant:
                selected.append(col)
        
        # å¦‚æœé€‰çš„ä¸å¤Ÿï¼Œè¡¥å……é«˜æ–¹å·®ç‰¹å¾
        for col in variances.index:
            if len(selected) >= max_features:
                break
            if col not in selected:
                selected.append(col)
        
        return selected
    
    # ==================== 1. ç¼ºå¤±å€¼å¤„ç† ====================
    
    def get_missing_info(self) -> pd.DataFrame:
        """
        è·å–ç¼ºå¤±å€¼ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç¼ºå¤±å€¼ç»Ÿè®¡çš„ DataFrame
        """
        total_rows = len(self.df)
        missing_count = self.df.isnull().sum()
        missing_percent = (missing_count / total_rows * 100).round(2)
        non_missing_count = total_rows - missing_count
        
        # ä½¿ç”¨åˆ—åä½œä¸ºç´¢å¼•
        info = pd.DataFrame({
            'Column': self.df.columns,
            'Missing': missing_count.values,
            'Missing%': missing_percent.values,
            'Non-Missing': non_missing_count.values,
            'Type': self.df.dtypes.values.astype(str)
        })
        
        # åªè¿”å›æœ‰ç¼ºå¤±å€¼çš„åˆ—ï¼ŒæŒ‰ç¼ºå¤±æ¯”ä¾‹æ’åº
        info = info[info['Missing'] > 0].sort_values('Missing%', ascending=False)
        info = info.reset_index(drop=True)
        
        return info
    
    def plot_missing_matrix(self, figsize: Tuple[int, int] = (14, 10), 
                             max_features: int = 15) -> plt.Figure:
        """
        ç»˜åˆ¶ç¼ºå¤±æ•°æ®çŸ©é˜µå›¾ï¼ˆå¸¦åˆ—åå’Œç¼ºå¤±æ¯”ä¾‹ï¼‰
        
        Args:
            figsize: å›¾å½¢å¤§å°
            max_features: æœ€å¤§æ˜¾ç¤ºç‰¹å¾æ•°
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        # è®¡ç®—æ¯åˆ—ç¼ºå¤±æ¯”ä¾‹
        missing_pct = (self.df.isnull().sum() / len(self.df) * 100).round(1)
        
        # æŒ‰ç¼ºå¤±æ¯”ä¾‹æ’åºï¼Œä¼˜å…ˆæ˜¾ç¤ºæœ‰ç¼ºå¤±çš„åˆ—
        sorted_cols = missing_pct.sort_values(ascending=False).index.tolist()
        
        # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
        display_cols = sorted_cols[:max_features]
        
        # åˆ›å»ºå¸¦åˆ—åæ ‡æ³¨çš„æ ‡ç­¾
        col_labels = []
        for col in display_cols:
            pct = missing_pct[col]
            if pct > 0:
                col_labels.append(f"{col}\n({pct:.1f}%)")
            else:
                col_labels.append(col)
        
        fig, ax = plt.subplots(figsize=figsize)
        
        # ç»˜åˆ¶çŸ©é˜µ
        plot_data = self.df[display_cols].isnull()
        
        # ä½¿ç”¨æ¸…æ™°çš„é…è‰²
        cmap = plt.cm.colors.ListedColormap(['#2ecc71', '#e74c3c'])  # ç»¿=æœ‰æ•°æ®, çº¢=ç¼ºå¤±
        
        im = ax.imshow(plot_data.values, aspect='auto', cmap=cmap, interpolation='nearest')
        
        # è®¾ç½®åˆ—æ ‡ç­¾
        ax.set_xticks(range(len(display_cols)))
        ax.set_xticklabels(col_labels, rotation=45, ha='right', fontsize=9)
        
        # è®¾ç½®è¡Œæ ‡ç­¾ï¼ˆåªæ˜¾ç¤ºéƒ¨åˆ†ï¼‰
        n_rows = len(self.df)
        if n_rows > 20:
            yticks = np.linspace(0, n_rows-1, 10, dtype=int)
            ax.set_yticks(yticks)
            ax.set_yticklabels(yticks)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, shrink=0.5, ticks=[0.25, 0.75])
        cbar.ax.set_yticklabels(['Present', 'Missing'])
        
        # æ·»åŠ ç¼ºå¤±ç»Ÿè®¡æ‘˜è¦
        total_missing = self.df.isnull().sum().sum()
        total_cells = self.df.shape[0] * self.df.shape[1]
        missing_ratio = total_missing / total_cells * 100
        
        title = get_display_text(
            f"ç¼ºå¤±å€¼çŸ©é˜µå›¾\næ€»ç¼ºå¤±: {total_missing:,} / {total_cells:,} ({missing_ratio:.2f}%)",
            f"Missing Data Matrix\nTotal Missing: {total_missing:,} / {total_cells:,} ({missing_ratio:.2f}%)"
        )
        ax.set_title(title, fontsize=13, fontweight='bold', pad=15)
        ax.set_xlabel(get_display_text("ç‰¹å¾ (ç¼ºå¤±æ¯”ä¾‹)", "Features (Missing %)"), fontsize=11)
        ax.set_ylabel(get_display_text("æ ·æœ¬ç´¢å¼•", "Sample Index"), fontsize=11)
        
        plt.tight_layout()
        return fig
    
    def plot_missing_correlation(self, figsize: Tuple[int, int] = (10, 8)) -> plt.Figure:
        """
        ç»˜åˆ¶ç¼ºå¤±å€¼ç›¸å…³æ€§çƒ­åŠ›å›¾
        
        Args:
            figsize: å›¾å½¢å¤§å°
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        # åªé€‰æ‹©æœ‰ç¼ºå¤±å€¼çš„åˆ—
        missing_cols = self.df.columns[self.df.isnull().any()].tolist()
        if len(missing_cols) < 2:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.text(0.5, 0.5, "ç¼ºå¤±å€¼åˆ—æ•°å°‘äº2ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§", 
                   ha='center', va='center', fontsize=12)
            ax.axis('off')
            return fig
        
        # è®¡ç®—ç¼ºå¤±å€¼ç›¸å…³æ€§
        missing_corr = self.df[missing_cols].isnull().corr()
        
        fig, ax = plt.subplots(figsize=figsize)
        sns.heatmap(missing_corr, annot=True, cmap='coolwarm', 
                    center=0, vmin=-1, vmax=1, ax=ax)
        ax.set_title("ç¼ºå¤±å€¼ç›¸å…³æ€§çƒ­åŠ›å›¾", fontsize=14)
        plt.tight_layout()
        return fig
    
    def fill_missing(self, strategy: str = 'auto', columns: List[str] = None) -> 'DataProcessor':
        """
        å¡«å……ç¼ºå¤±å€¼
        
        Args:
            strategy: å¡«å……ç­–ç•¥
                - 'auto': è‡ªåŠ¨é€‰æ‹©ï¼ˆæ•°å€¼ç”¨ä¸­ä½æ•°ï¼Œç±»åˆ«ç”¨ä¼—æ•°ï¼‰
                - 'median': ä¸­ä½æ•°å¡«å……
                - 'mean': å‡å€¼å¡«å……
                - 'mode': ä¼—æ•°å¡«å……
                - 'knn': KNNå¡«å……ï¼ˆä»…æ•°å€¼å‹ï¼‰
                - 'drop': åˆ é™¤ç¼ºå¤±è¡Œ
            columns: è¦å¤„ç†çš„åˆ—ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—
            
        Returns:
            selfï¼Œæ”¯æŒé“¾å¼è°ƒç”¨
        """
        cols = columns if columns else self.df.columns.tolist()
        
        for col in cols:
            if self.df[col].isnull().sum() == 0:
                continue
                
            if strategy == 'auto':
                if self.df[col].dtype in ['float64', 'int64']:
                    self.df[col] = self.df[col].fillna(self.df[col].median())
                    self.processing_log.append(f"åˆ— '{col}' ä½¿ç”¨ä¸­ä½æ•°å¡«å……")
                else:
                    self.df[col] = self.df[col].fillna(self.df[col].mode()[0])
                    self.processing_log.append(f"åˆ— '{col}' ä½¿ç”¨ä¼—æ•°å¡«å……")
                    
            elif strategy == 'median':
                self.df[col] = self.df[col].fillna(self.df[col].median())
                self.processing_log.append(f"åˆ— '{col}' ä½¿ç”¨ä¸­ä½æ•°å¡«å……")
                
            elif strategy == 'mean':
                self.df[col] = self.df[col].fillna(self.df[col].mean())
                self.processing_log.append(f"åˆ— '{col}' ä½¿ç”¨å‡å€¼å¡«å……")
                
            elif strategy == 'mode':
                self.df[col] = self.df[col].fillna(self.df[col].mode()[0])
                self.processing_log.append(f"åˆ— '{col}' ä½¿ç”¨ä¼—æ•°å¡«å……")
                
            elif strategy == 'drop':
                self.df.dropna(subset=[col], inplace=True)
                self.processing_log.append(f"åˆ é™¤åˆ— '{col}' çš„ç¼ºå¤±è¡Œ")
        
        if strategy == 'knn':
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                imputer = KNNImputer(n_neighbors=5)
                self.df[numeric_cols] = imputer.fit_transform(self.df[numeric_cols])
                self.processing_log.append("ä½¿ç”¨KNNå¡«å……æ•°å€¼å‹ç¼ºå¤±å€¼")
        
        return self
    
    # ==================== 2. å¼‚å¸¸å€¼å¤„ç† ====================
    
    def detect_outliers_iqr(self, column: str) -> Dict:
        """
        ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        
        Args:
            column: åˆ—å
            
        Returns:
            åŒ…å«å¼‚å¸¸å€¼ä¿¡æ¯çš„å­—å…¸
        """
        Q1 = self.df[column].quantile(0.25)
        Q3 = self.df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = self.df[(self.df[column] < lower_bound) | (self.df[column] > upper_bound)]
        
        return {
            'column': column,
            'Q1': Q1,
            'Q3': Q3,
            'IQR': IQR,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'outlier_count': len(outliers),
            'outlier_percent': round(len(outliers) / len(self.df) * 100, 2)
        }
    
    def detect_outliers_zscore(self, column: str, threshold: float = 3.0) -> Dict:
        """
        ä½¿ç”¨Z-Scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        
        Args:
            column: åˆ—å
            threshold: Z-Scoreé˜ˆå€¼ï¼Œé»˜è®¤3
            
        Returns:
            åŒ…å«å¼‚å¸¸å€¼ä¿¡æ¯çš„å­—å…¸
        """
        z_scores = np.abs(stats.zscore(self.df[column].dropna()))
        outlier_mask = z_scores > threshold
        
        return {
            'column': column,
            'threshold': threshold,
            'outlier_count': outlier_mask.sum(),
            'outlier_percent': round(outlier_mask.sum() / len(self.df) * 100, 2)
        }
    
    def plot_boxplot(self, columns: List[str] = None, figsize: Tuple[int, int] = (14, 8),
                      max_features: int = MAX_DISPLAY_FEATURES, 
                      normalize: bool = True) -> plt.Figure:
        """
        ç»˜åˆ¶ç¾è§‚çš„ç®±çº¿å›¾ï¼ˆæ¨ªå‘å±•ç¤ºï¼Œè‡ªåŠ¨é€‰æ‹©ä»£è¡¨æ€§ç‰¹å¾ï¼‰
        
        Args:
            columns: è¦ç»˜åˆ¶çš„åˆ—ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
            figsize: å›¾å½¢å¤§å°
            max_features: æœ€å¤§æ˜¾ç¤ºç‰¹å¾æ•°
            normalize: æ˜¯å¦å½’ä¸€åŒ–æ•°æ®ï¼ˆä¾¿äºä¸åŒå°ºåº¦ç‰¹å¾æ¯”è¾ƒï¼‰
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        if columns is None:
            all_numeric = self.df.select_dtypes(include=[np.number]).columns.tolist()
            columns = self._select_representative_features(all_numeric, max_features)
        
        n_cols = len(columns)
        if n_cols == 0:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.text(0.5, 0.5, "No numeric columns to plot", ha='center', va='center', fontsize=14)
            ax.axis('off')
            return fig
        
        # ä½¿ç”¨æ¨ªå‘ç®±çº¿å›¾ï¼Œæ›´ç¾è§‚
        fig, ax = plt.subplots(figsize=figsize)
        
        # å‡†å¤‡æ•°æ®
        df_plot = self.df[columns].copy()
        
        # ç»Ÿè®¡åŸå§‹å¼‚å¸¸å€¼ä¿¡æ¯ï¼ˆåœ¨å½’ä¸€åŒ–ä¹‹å‰ï¼‰
        outlier_counts = {}
        for col in columns:
            outlier_info = self.detect_outliers_iqr(col)
            outlier_counts[col] = outlier_info['outlier_count']
        
        # å½’ä¸€åŒ–å¤„ç†ï¼ˆMin-Max æ ‡å‡†åŒ–åˆ° 0-1ï¼‰
        if normalize:
            for col in columns:
                col_min = df_plot[col].min()
                col_max = df_plot[col].max()
                if col_max > col_min:
                    df_plot[col] = (df_plot[col] - col_min) / (col_max - col_min)
                else:
                    df_plot[col] = 0.5  # å¸¸é‡åˆ—
        
        # åˆ›å»ºç¾è§‚çš„ç®±çº¿å›¾
        colors = sns.color_palette("husl", n_cols)
        
        bp = ax.boxplot(
            [df_plot[col].dropna() for col in columns],
            tick_labels=columns,
            vert=False,  # æ¨ªå‘
            patch_artist=True,
            notch=True,  # æ˜¾ç¤ºç½®ä¿¡åŒºé—´
            showfliers=True,
            flierprops=dict(marker='o', markerfacecolor='red', markersize=4, alpha=0.5),
            medianprops=dict(color='darkblue', linewidth=2),
            whiskerprops=dict(color='gray', linewidth=1.5),
            capprops=dict(color='gray', linewidth=1.5)
        )
        
        # è®¾ç½®ç®±ä½“é¢œè‰²
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
            patch.set_edgecolor('black')
        
        # æ·»åŠ å¼‚å¸¸å€¼æ•°é‡æ ‡æ³¨ï¼ˆä½¿ç”¨åŸå§‹æ•°æ®çš„ç»Ÿè®¡ï¼‰
        for i, col in enumerate(columns):
            outlier_count = outlier_counts[col]
            if outlier_count > 0:
                # æ ‡æ³¨åœ¨å›¾çš„å³ä¾§
                ax.annotate(f'{outlier_count}', 
                           xy=(1.05, i + 1),
                           fontsize=9, color='red', alpha=0.8,
                           fontweight='bold')
        
        # è®¾ç½®æ ‡ç­¾
        xlabel = get_display_text('å½’ä¸€åŒ–æ•°å€¼ (0-1)', 'Normalized Value (0-1)') if normalize else get_display_text('æ•°å€¼', 'Value')
        ax.set_xlabel(xlabel, fontsize=12)
        
        title = get_display_text(
            'ç®±çº¿å›¾ - å¼‚å¸¸å€¼æ£€æµ‹\n(å·²å½’ä¸€åŒ–ï¼Œçº¢è‰²æ•°å­—ä¸ºå¼‚å¸¸å€¼æ•°é‡)', 
            'Box Plot - Outlier Detection\n(Normalized, red numbers = outlier count)'
        ) if normalize else get_display_text(
            'ç®±çº¿å›¾ - å¼‚å¸¸å€¼æ£€æµ‹', 
            'Box Plot - Outlier Detection'
        )
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3, linestyle='--')
        
        if normalize:
            ax.set_xlim(-0.05, 1.15)  # ç•™å‡ºæ ‡æ³¨ç©ºé—´
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        return fig
    
    def plot_violin(self, columns: List[str] = None, figsize: Tuple[int, int] = (12, 6)) -> plt.Figure:
        """
        ç»˜åˆ¶å°æç´å›¾
        
        Args:
            columns: è¦ç»˜åˆ¶çš„åˆ—
            figsize: å›¾å½¢å¤§å°
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()[:6]
        
        n_cols = len(columns)
        if n_cols == 0:
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.text(0.5, 0.5, "æ²¡æœ‰æ•°å€¼å‹åˆ—å¯ç»˜åˆ¶", ha='center', va='center')
            ax.axis('off')
            return fig
        
        fig, axes = plt.subplots(1, n_cols, figsize=figsize)
        axes = [axes] if n_cols == 1 else axes
        
        for i, col in enumerate(columns):
            sns.violinplot(y=self.df[col], ax=axes[i])
            axes[i].set_title(f'{col}')
        
        plt.suptitle("å°æç´å›¾ - æ•°æ®åˆ†å¸ƒ", fontsize=14)
        plt.tight_layout()
        return fig
    
    def handle_outliers(self, columns: List[str] = None, method: str = 'cap') -> 'DataProcessor':
        """
        å¤„ç†å¼‚å¸¸å€¼
        
        Args:
            columns: è¦å¤„ç†çš„åˆ—ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰æ•°å€¼åˆ—
            method: å¤„ç†æ–¹æ³•
                - 'cap': ç›–å¸½æ³•ï¼ˆæˆªæ–­åˆ°IQRè¾¹ç•Œï¼‰
                - 'drop': åˆ é™¤å¼‚å¸¸å€¼è¡Œ
                - 'median': ç”¨ä¸­ä½æ•°æ›¿æ¢
                
        Returns:
            selfï¼Œæ”¯æŒé“¾å¼è°ƒç”¨
        """
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        for col in columns:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            
            if method == 'cap':
                self.df[col] = self.df[col].clip(lower=lower, upper=upper)
                self.processing_log.append(f"åˆ— '{col}' ä½¿ç”¨ç›–å¸½æ³•å¤„ç†å¼‚å¸¸å€¼")
                
            elif method == 'drop':
                mask = (self.df[col] >= lower) & (self.df[col] <= upper)
                self.df = self.df[mask]
                self.processing_log.append(f"åˆ é™¤åˆ— '{col}' çš„å¼‚å¸¸å€¼è¡Œ")
                
            elif method == 'median':
                median = self.df[col].median()
                mask = (self.df[col] < lower) | (self.df[col] > upper)
                self.df.loc[mask, col] = median
                self.processing_log.append(f"åˆ— '{col}' ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼")
        
        return self
    
    def encode_categorical(self, strategy: str = 'auto', 
                           columns: List[str] = None,
                           target_column: str = None) -> 'DataProcessor':
        """
        ç¼–ç åˆ†ç±»å˜é‡ï¼ˆå­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºæ•°å€¼ï¼‰
        
        Args:
            strategy: ç¼–ç ç­–ç•¥
                - 'auto': è‡ªåŠ¨é€‰æ‹©ï¼ˆå”¯ä¸€å€¼â‰¤2ç”¨labelï¼Œ3-10ç”¨onehotï¼Œ>10ç”¨labelï¼‰
                - 'label': æ ‡ç­¾ç¼–ç ï¼ˆ0,1,2,3...ï¼‰
                - 'onehot': ç‹¬çƒ­ç¼–ç ï¼ˆæ¯ä¸ªç±»åˆ«ä¸€åˆ—ï¼‰
                - 'target': ç›®æ ‡ç¼–ç ï¼ˆç”¨ç›®æ ‡å˜é‡å‡å€¼æ›¿æ¢ï¼Œéœ€è¦target_columnï¼‰
            columns: è¦ç¼–ç çš„åˆ—ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å­—ç¬¦ä¸²/å¯¹è±¡ç±»å‹åˆ—
            target_column: ç›®æ ‡åˆ—åï¼ˆä»…targetç¼–ç éœ€è¦ï¼‰
            
        Returns:
            selfï¼Œæ”¯æŒé“¾å¼è°ƒç”¨
        """
        from sklearn.preprocessing import LabelEncoder
        
        # è·å–éœ€è¦ç¼–ç çš„åˆ—
        if columns is None:
            columns = self.df.select_dtypes(include=['object', 'category']).columns.tolist()
            # æ’é™¤ç›®æ ‡åˆ—
            if target_column and target_column in columns:
                columns.remove(target_column)
        
        if not columns:
            self.processing_log.append("æ²¡æœ‰éœ€è¦ç¼–ç çš„åˆ†ç±»åˆ—")
            return self
        
        encoded_info = []
        
        for col in columns:
            unique_count = self.df[col].nunique()
            
            # ç¡®å®šä½¿ç”¨çš„ç­–ç•¥
            if strategy == 'auto':
                if unique_count <= 2:
                    col_strategy = 'label'
                elif unique_count <= 10:
                    col_strategy = 'onehot'
                else:
                    col_strategy = 'label'  # é«˜åŸºæ•°ç”¨ labelï¼Œé¿å…ç»´åº¦çˆ†ç‚¸
            else:
                col_strategy = strategy
            
            # æ‰§è¡Œç¼–ç 
            if col_strategy == 'label':
                le = LabelEncoder()
                # å¤„ç†ç¼ºå¤±å€¼
                mask = self.df[col].notna()
                self.df.loc[mask, col] = le.fit_transform(self.df.loc[mask, col].astype(str))
                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
                encoded_info.append(f"'{col}' (Label, {unique_count}ç±»)")
                
            elif col_strategy == 'onehot':
                # ç‹¬çƒ­ç¼–ç 
                dummies = pd.get_dummies(self.df[col], prefix=col, dummy_na=False)
                self.df = pd.concat([self.df.drop(col, axis=1), dummies], axis=1)
                encoded_info.append(f"'{col}' (OneHot, {unique_count}ç±»â†’{len(dummies.columns)}åˆ—)")
                
            elif col_strategy == 'target' and target_column:
                # ç›®æ ‡ç¼–ç 
                if target_column in self.df.columns:
                    target_mean = self.df.groupby(col)[target_column].mean()
                    self.df[col] = self.df[col].map(target_mean)
                    encoded_info.append(f"'{col}' (Target, {unique_count}ç±»)")
                else:
                    # fallback to label
                    le = LabelEncoder()
                    mask = self.df[col].notna()
                    self.df.loc[mask, col] = le.fit_transform(self.df.loc[mask, col].astype(str))
                    self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
                    encoded_info.append(f"'{col}' (Label-fallback, {unique_count}ç±»)")
        
        if encoded_info:
            self.processing_log.append(f"åˆ†ç±»å˜é‡ç¼–ç : {', '.join(encoded_info)}")
        
        return self
    
    def get_categorical_columns(self) -> Dict[str, int]:
        """
        è·å–æ‰€æœ‰åˆ†ç±»åˆ—åŠå…¶å”¯ä¸€å€¼æ•°é‡
        
        Returns:
            {åˆ—å: å”¯ä¸€å€¼æ•°é‡}
        """
        cat_cols = self.df.select_dtypes(include=['object', 'category']).columns
        return {col: self.df[col].nunique() for col in cat_cols}
    
    # ==================== 3. æ•°æ®åˆ†å¸ƒæ ¡æ­£ ====================
    
    def plot_distribution(self, columns: List[str] = None, figsize: Tuple[int, int] = (14, 10),
                           max_features: int = 6) -> plt.Figure:
        """
        ç»˜åˆ¶æ•°æ®åˆ†å¸ƒå›¾ï¼ˆç›´æ–¹å›¾ + KDEï¼‰
        
        Args:
            columns: è¦ç»˜åˆ¶çš„åˆ—
            figsize: å›¾å½¢å¤§å°
            max_features: æœ€å¤§æ˜¾ç¤ºç‰¹å¾æ•°
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        if columns is None:
            all_numeric = self.df.select_dtypes(include=[np.number]).columns.tolist()
            columns = self._select_representative_features(all_numeric, max_features)
        
        n_cols = len(columns)
        if n_cols == 0:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.text(0.5, 0.5, "No numeric columns to plot", ha='center', va='center', fontsize=14)
            ax.axis('off')
            return fig
        
        n_rows = (n_cols + 1) // 2
        fig, axes = plt.subplots(n_rows, 2, figsize=figsize)
        axes = axes.flatten() if n_cols > 1 else [axes]
        
        colors = sns.color_palette("viridis", n_cols)
        
        for i, col in enumerate(columns):
            data = self.df[col].dropna()
            
            # ä½¿ç”¨ç¾è§‚çš„ç›´æ–¹å›¾+KDE
            sns.histplot(data, kde=True, ax=axes[i], color=colors[i], 
                        edgecolor='white', alpha=0.7, linewidth=0.5)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            skewness = data.skew()
            mean_val = data.mean()
            
            # æ·»åŠ å‡å€¼çº¿
            axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=1.5, alpha=0.8)
            
            # è®¾ç½®æ ‡é¢˜ï¼ˆä½¿ç”¨è‹±æ–‡é¿å…å­—ä½“é—®é¢˜ï¼‰
            skew_status = "Right-skewed" if skewness > 0.5 else ("Left-skewed" if skewness < -0.5 else "Normal")
            axes[i].set_title(f'{col}\nSkewness: {skewness:.2f} ({skew_status})', fontsize=11)
            axes[i].set_xlabel('')
            axes[i].grid(axis='y', alpha=0.3, linestyle='--')
        
        for j in range(i + 1, len(axes)):
            axes[j].axis('off')
        
        plt.suptitle(get_display_text(
            "æ•°æ®åˆ†å¸ƒåˆ†æ\n(æŒ‰æ–¹å·®é€‰å–ä»£è¡¨æ€§ç‰¹å¾)", 
            "Distribution Analysis\n(Top features by variance)"
        ), fontsize=14, fontweight='bold')
        plt.tight_layout()
        return fig
    
    def plot_qq(self, column: str, figsize: Tuple[int, int] = (8, 6)) -> plt.Figure:
        """
        ç»˜åˆ¶Q-Qå›¾
        
        Args:
            column: åˆ—å
            figsize: å›¾å½¢å¤§å°
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        fig, ax = plt.subplots(figsize=figsize)
        stats.probplot(self.df[column].dropna(), dist="norm", plot=ax)
        ax.set_title(f"Q-Qå›¾ - {column}")
        plt.tight_layout()
        return fig
    
    def transform_log(self, columns: List[str]) -> 'DataProcessor':
        """
        å¯¹æ•°å˜æ¢ï¼ˆé€‚ç”¨äºå³åæ•°æ®ï¼‰
        
        Args:
            columns: è¦å˜æ¢çš„åˆ—
            
        Returns:
            selfï¼Œæ”¯æŒé“¾å¼è°ƒç”¨
        """
        for col in columns:
            # ç¡®ä¿æ•°æ®ä¸ºæ­£æ•°
            min_val = self.df[col].min()
            if min_val <= 0:
                shift = abs(min_val) + 1
                self.df[f'{col}_log'] = np.log1p(self.df[col] + shift)
            else:
                self.df[f'{col}_log'] = np.log1p(self.df[col])
            self.processing_log.append(f"åˆ— '{col}' è¿›è¡Œå¯¹æ•°å˜æ¢ï¼Œç”Ÿæˆ '{col}_log'")
        
        return self
    
    def transform_boxcox(self, columns: List[str]) -> 'DataProcessor':
        """
        Box-Coxå˜æ¢
        
        Args:
            columns: è¦å˜æ¢çš„åˆ—
            
        Returns:
            selfï¼Œæ”¯æŒé“¾å¼è°ƒç”¨
        """
        for col in columns:
            # Box-Coxè¦æ±‚æ•°æ®ä¸ºæ­£æ•°
            data = self.df[col].dropna()
            if data.min() <= 0:
                data = data + abs(data.min()) + 1
            
            transformed, lambda_param = stats.boxcox(data)
            self.df[f'{col}_boxcox'] = np.nan
            self.df.loc[self.df[col].notna(), f'{col}_boxcox'] = transformed
            self.processing_log.append(f"åˆ— '{col}' è¿›è¡ŒBox-Coxå˜æ¢ (Î»={lambda_param:.2f})")
        
        return self
    
    # ==================== 4. ç›¸å…³æ€§åˆ†æ ====================
    
    def plot_correlation_heatmap(self, columns: List[str] = None, 
                                  figsize: Tuple[int, int] = (12, 10),
                                  max_features: int = 12) -> plt.Figure:
        """
        ç»˜åˆ¶ç¾è§‚çš„ç›¸å…³æ€§çƒ­åŠ›å›¾
        
        Args:
            columns: è¦åˆ†æçš„åˆ—
            figsize: å›¾å½¢å¤§å°
            max_features: æœ€å¤§æ˜¾ç¤ºç‰¹å¾æ•°
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        if columns is None:
            all_numeric = self.df.select_dtypes(include=[np.number]).columns.tolist()
            columns = self._select_representative_features(all_numeric, max_features)
        
        if len(columns) > max_features:
            columns = columns[:max_features]
        
        corr = self.df[columns].corr()
        
        fig, ax = plt.subplots(figsize=figsize)
        mask = np.triu(np.ones_like(corr, dtype=bool))
        
        # ä½¿ç”¨æ›´ç¾è§‚çš„é…è‰²
        cmap = sns.diverging_palette(250, 10, as_cmap=True)
        
        sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, 
                    center=0, vmin=-1, vmax=1, ax=ax, fmt='.2f',
                    square=True, linewidths=0.5, linecolor='white',
                    annot_kws={"size": 9},
                    cbar_kws={"shrink": 0.8, "label": "Correlation"})
        
        ax.set_title(get_display_text(
            "ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾\n(æŒ‰æ–¹å·®é€‰å–ä»£è¡¨æ€§ç‰¹å¾)", 
            "Feature Correlation Heatmap\n(Top features by variance)"
        ), fontsize=14, fontweight='bold', pad=20)
        
        # æ—‹è½¬æ ‡ç­¾ä½¿å…¶æ›´æ˜“è¯»
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        return fig
    
    def plot_pairplot(self, columns: List[str] = None, hue: str = None) -> plt.Figure:
        """
        ç»˜åˆ¶é…å¯¹å›¾
        
        Args:
            columns: è¦ç»˜åˆ¶çš„åˆ—ï¼ˆå»ºè®®ä¸è¶…è¿‡5ä¸ªï¼‰
            hue: ç”¨äºåˆ†ç»„çš„åˆ—
            
        Returns:
            matplotlib Figure å¯¹è±¡
        """
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()[:4]
        
        if hue and hue not in columns:
            plot_data = self.df[columns + [hue]]
        else:
            plot_data = self.df[columns]
        
        fig = sns.pairplot(plot_data, hue=hue, diag_kind='kde')
        fig.fig.suptitle("é…å¯¹æ•£ç‚¹å›¾", y=1.02)
        return fig.fig
    
    def get_high_correlation_pairs(
        self, 
        threshold: float = None, 
        top_k: int = None,
        adaptive: bool = True,
        min_pairs: int = 5
    ) -> pd.DataFrame:
        """
        è·å–é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
        
        Args:
            threshold: ç›¸å…³ç³»æ•°é˜ˆå€¼ï¼ˆNone æ—¶ä½¿ç”¨è‡ªé€‚åº”ï¼‰
            top_k: è¿”å›å‰ k ä¸ªæœ€é«˜ç›¸å…³æ€§å¯¹ï¼ˆä¼˜å…ˆäº thresholdï¼‰
            adaptive: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼ˆå½“ threshold å’Œ top_k éƒ½ä¸º None æ—¶ï¼‰
            min_pairs: è‡ªé€‚åº”æ¨¡å¼ä¸‹æœ€å°‘è¿”å›çš„ç‰¹å¾å¯¹æ•°é‡
            
        Returns:
            é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹çš„ DataFrame
        """
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) < 2:
            return pd.DataFrame(columns=['ç‰¹å¾1', 'ç‰¹å¾2', 'ç›¸å…³ç³»æ•°'])
        
        corr = self.df[numeric_cols].corr()
        
        # æ”¶é›†æ‰€æœ‰ç‰¹å¾å¯¹åŠå…¶ç›¸å…³ç³»æ•°
        all_pairs = []
        for i in range(len(corr.columns)):
            for j in range(i + 1, len(corr.columns)):
                corr_val = corr.iloc[i, j]
                if not np.isnan(corr_val):
                    all_pairs.append({
                        'ç‰¹å¾1': corr.columns[i],
                        'ç‰¹å¾2': corr.columns[j],
                        'ç›¸å…³ç³»æ•°': round(corr_val, 3)
                    })
        
        if not all_pairs:
            return pd.DataFrame(columns=['ç‰¹å¾1', 'ç‰¹å¾2', 'ç›¸å…³ç³»æ•°'])
        
        # æŒ‰ç»å¯¹å€¼æ’åº
        df_pairs = pd.DataFrame(all_pairs)
        df_pairs = df_pairs.sort_values('ç›¸å…³ç³»æ•°', key=abs, ascending=False).reset_index(drop=True)
        
        # æ¨¡å¼1: top_k - è¿”å›å‰ k ä¸ª
        if top_k is not None:
            return df_pairs.head(top_k)
        
        # æ¨¡å¼2: å›ºå®šé˜ˆå€¼
        if threshold is not None:
            result = df_pairs[df_pairs['ç›¸å…³ç³»æ•°'].abs() >= threshold]
            # å¦‚æœç»“æœä¸ºç©ºä¸”å¼€å¯è‡ªé€‚åº”ï¼Œé™çº§åˆ°è‡ªé€‚åº”æ¨¡å¼
            if len(result) == 0 and adaptive:
                pass  # ç»§ç»­åˆ°è‡ªé€‚åº”æ¨¡å¼
            else:
                return result
        
        # æ¨¡å¼3: è‡ªé€‚åº”é˜ˆå€¼
        if adaptive:
            # ç¡®ä¿è‡³å°‘è¿”å› min_pairs ä¸ªç»“æœ
            if len(df_pairs) <= min_pairs:
                return df_pairs
            
            # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼ï¼šå–å‰ min_pairs ä¸ªçš„æœ€å°ç»å¯¹å€¼ï¼Œæˆ–è€…ä½¿ç”¨åˆ†ä½æ•°
            abs_corrs = df_pairs['ç›¸å…³ç³»æ•°'].abs()
            
            # ç­–ç•¥1: ä½¿ç”¨ 75 åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
            q75_threshold = abs_corrs.quantile(0.75)
            
            # ç­–ç•¥2: ç¡®ä¿è‡³å°‘æœ‰ min_pairs ä¸ªç»“æœ
            if len(df_pairs) > min_pairs:
                min_threshold = abs_corrs.iloc[min_pairs - 1]
            else:
                min_threshold = abs_corrs.min()
            
            # å–ä¸¤è€…ä¸­è¾ƒå°çš„ï¼ˆè¿”å›æ›´å¤šç»“æœï¼‰
            adaptive_threshold = min(q75_threshold, min_threshold)
            
            result = df_pairs[abs_corrs >= adaptive_threshold]
            
            # å…œåº•ï¼šå¦‚æœä»ç„¶å¤ªå°‘ï¼Œè¿”å›å‰ min_pairs ä¸ª
            if len(result) < min_pairs:
                return df_pairs.head(min_pairs)
            
            return result
        
        # é»˜è®¤è¿”å›æ‰€æœ‰
        return df_pairs
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def get_processing_log(self) -> List[str]:
        """è·å–å¤„ç†æ—¥å¿—"""
        return self.processing_log
    
    def get_summary(self) -> pd.DataFrame:
        """è·å–æ•°æ®æ‘˜è¦"""
        summary = self.df.describe(include='all').T
        summary['ç¼ºå¤±å€¼'] = self.df.isnull().sum()
        summary['ç¼ºå¤±ç‡(%)'] = (self.df.isnull().sum() / len(self.df) * 100).round(2)
        return summary
    
    def generate_report(self, include_recommendations: bool = True) -> dict:
        """
        ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š
        
        Args:
            include_recommendations: æ˜¯å¦åŒ…å«å¤„ç†å»ºè®®
            
        Returns:
            åŒ…å«æŠ¥å‘Šå„éƒ¨åˆ†çš„å­—å…¸
        """
        report = {
            'overview': {},
            'missing_analysis': {},
            'outlier_analysis': {},
            'distribution_analysis': {},
            'correlation_analysis': {},
            'recommendations': [],
            'processing_log': self.processing_log,
            'markdown': '',
            'llm_prompt': ''
        }
        
        # 1. æ•°æ®æ¦‚è§ˆ
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns.tolist()
        
        report['overview'] = {
            'total_rows': len(self.df),
            'total_columns': len(self.df.columns),
            'numeric_columns': len(numeric_cols),
            'categorical_columns': len(categorical_cols),
            'memory_usage_mb': round(self.df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            'column_names': self.df.columns.tolist()
        }
        
        # 2. ç¼ºå¤±å€¼åˆ†æ
        missing_info = self.get_missing_info()
        missing_cols = missing_info[missing_info['Missing'] > 0]
        report['missing_analysis'] = {
            'total_missing_cells': int(self.df.isnull().sum().sum()),
            'missing_rate': round(self.df.isnull().sum().sum() / self.df.size * 100, 2),
            'columns_with_missing': len(missing_cols),
            'details': missing_cols.to_dict('records') if len(missing_cols) > 0 else []
        }
        
        # 3. å¼‚å¸¸å€¼åˆ†æ
        outlier_info = []
        for col in numeric_cols:
            outlier_result = self.detect_outliers_iqr(col)
            outlier_count = outlier_result['outlier_count']
            if outlier_count > 0:
                # è·å–å¼‚å¸¸å€¼çš„å®é™…æ•°æ®
                lower_bound = outlier_result['lower_bound']
                upper_bound = outlier_result['upper_bound']
                outlier_values = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)][col]
                
                outlier_info.append({
                    'column': col,
                    'outlier_count': outlier_count,
                    'outlier_rate': round(outlier_count / len(self.df) * 100, 2),
                    'min_outlier': round(float(outlier_values.min()), 3) if len(outlier_values) > 0 else None,
                    'max_outlier': round(float(outlier_values.max()), 3) if len(outlier_values) > 0 else None
                })
        report['outlier_analysis'] = {
            'columns_with_outliers': len(outlier_info),
            'details': outlier_info
        }
        
        # 4. åˆ†å¸ƒåˆ†æ
        dist_info = []
        for col in numeric_cols[:10]:  # é™åˆ¶æ•°é‡
            try:
                skewness = float(self.df[col].skew())
                kurtosis = float(self.df[col].kurtosis())
                dist_info.append({
                    'column': col,
                    'mean': round(self.df[col].mean(), 3),
                    'std': round(self.df[col].std(), 3),
                    'skewness': round(skewness, 3),
                    'kurtosis': round(kurtosis, 3),
                    'distribution_type': 'å³å' if skewness > 1 else ('å·¦å' if skewness < -1 else 'è¿‘ä¼¼æ­£æ€')
                })
            except:
                pass
        report['distribution_analysis'] = {
            'analyzed_columns': len(dist_info),
            'details': dist_info
        }
        
        # 5. ç›¸å…³æ€§åˆ†æ
        corr_pairs = self.get_high_correlation_pairs(top_k=10)
        report['correlation_analysis'] = {
            'high_correlation_pairs': corr_pairs.to_dict('records') if len(corr_pairs) > 0 else [],
            'max_correlation': round(corr_pairs['ç›¸å…³ç³»æ•°'].abs().max(), 3) if len(corr_pairs) > 0 else 0
        }
        
        # 6. ç”Ÿæˆå»ºè®®
        if include_recommendations:
            recommendations = []
            
            # ç¼ºå¤±å€¼å»ºè®®
            if report['missing_analysis']['missing_rate'] > 30:
                recommendations.append("âš ï¸ æ•°æ®ç¼ºå¤±ç‡è¾ƒé«˜(>30%)ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æ¥æºæˆ–è€ƒè™‘åˆ é™¤é«˜ç¼ºå¤±åˆ—")
            elif report['missing_analysis']['missing_rate'] > 5:
                recommendations.append("ğŸ’¡ å­˜åœ¨ä¸€å®šç¼ºå¤±å€¼ï¼Œå»ºè®®ä½¿ç”¨ KNN æˆ–ä¸­ä½æ•°å¡«å……")
            
            # å¼‚å¸¸å€¼å»ºè®®
            if len(outlier_info) > len(numeric_cols) * 0.5:
                recommendations.append("âš ï¸ å¤šæ•°æ•°å€¼åˆ—å­˜åœ¨å¼‚å¸¸å€¼ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–ä½¿ç”¨æˆªæ–­å¤„ç†")
            
            # åˆ†å¸ƒå»ºè®®
            skewed_cols = [d['column'] for d in dist_info if abs(d.get('skewness', 0)) > 1]
            if skewed_cols:
                recommendations.append(f"ğŸ’¡ ä»¥ä¸‹åˆ—åˆ†å¸ƒåæ–œï¼Œå»ºè®®è¿›è¡Œå¯¹æ•°æˆ–Box-Coxå˜æ¢: {', '.join(skewed_cols[:5])}")
            
            # ç›¸å…³æ€§å»ºè®®
            if report['correlation_analysis']['max_correlation'] > 0.9:
                recommendations.append("âš ï¸ å­˜åœ¨é«˜åº¦ç›¸å…³ç‰¹å¾(>0.9)ï¼Œå»ºè®®è¿›è¡Œç‰¹å¾é€‰æ‹©æˆ–PCAé™ç»´")
            
            if not recommendations:
                recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç›´æ¥ç”¨äºå»ºæ¨¡")
            
            report['recommendations'] = recommendations
        
        # 7. ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š
        report['markdown'] = self._generate_markdown_report(report)
        
        # 8. ç”Ÿæˆ LLM åˆ†ææç¤ºè¯
        report['llm_prompt'] = self._generate_llm_prompt(report)
        
        return report
    
    def _generate_markdown_report(self, report: dict) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
        md = []
        md.append("# ğŸ“Š æ•°æ®åˆ†ææŠ¥å‘Š\n")
        
        # æ¦‚è§ˆ
        ov = report['overview']
        md.append("## 1. æ•°æ®æ¦‚è§ˆ")
        md.append(f"- **æ€»è¡Œæ•°**: {ov['total_rows']:,}")
        md.append(f"- **æ€»åˆ—æ•°**: {ov['total_columns']}")
        md.append(f"- **æ•°å€¼åˆ—**: {ov['numeric_columns']} åˆ—")
        md.append(f"- **åˆ†ç±»åˆ—**: {ov['categorical_columns']} åˆ—")
        md.append(f"- **å†…å­˜å ç”¨**: {ov['memory_usage_mb']} MB\n")
        
        # ç¼ºå¤±å€¼
        ma = report['missing_analysis']
        md.append("## 2. ç¼ºå¤±å€¼åˆ†æ")
        md.append(f"- **æ€»ç¼ºå¤±å•å…ƒæ ¼**: {ma['total_missing_cells']:,}")
        md.append(f"- **æ•´ä½“ç¼ºå¤±ç‡**: {ma['missing_rate']}%")
        md.append(f"- **å«ç¼ºå¤±å€¼çš„åˆ—æ•°**: {ma['columns_with_missing']}")
        if ma['details']:
            md.append("\n| åˆ—å | ç¼ºå¤±æ•°é‡ | ç¼ºå¤±ç‡ |")
            md.append("|------|----------|--------|")
            for d in ma['details'][:10]:
                md.append(f"| {d.get('Column', 'N/A')} | {d.get('Missing', 0)} | {d.get('Missing%', 0)}% |")
        md.append("")
        
        # å¼‚å¸¸å€¼
        oa = report['outlier_analysis']
        md.append("## 3. å¼‚å¸¸å€¼åˆ†æ")
        md.append(f"- **å«å¼‚å¸¸å€¼çš„åˆ—æ•°**: {oa['columns_with_outliers']}")
        if oa['details']:
            md.append("\n| åˆ—å | å¼‚å¸¸å€¼æ•°é‡ | å¼‚å¸¸ç‡ |")
            md.append("|------|------------|--------|")
            for d in oa['details'][:10]:
                md.append(f"| {d['column']} | {d['outlier_count']} | {d['outlier_rate']}% |")
        md.append("")
        
        # åˆ†å¸ƒ
        da = report['distribution_analysis']
        md.append("## 4. æ•°æ®åˆ†å¸ƒ")
        if da['details']:
            md.append("\n| åˆ—å | å‡å€¼ | æ ‡å‡†å·® | ååº¦ | åˆ†å¸ƒç±»å‹ |")
            md.append("|------|------|--------|------|----------|")
            for d in da['details'][:10]:
                md.append(f"| {d['column']} | {d['mean']} | {d['std']} | {d['skewness']} | {d['distribution_type']} |")
        md.append("")
        
        # ç›¸å…³æ€§
        ca = report['correlation_analysis']
        md.append("## 5. ç›¸å…³æ€§åˆ†æ")
        md.append(f"- **æœ€é«˜ç›¸å…³ç³»æ•°**: {ca['max_correlation']}")
        if ca['high_correlation_pairs']:
            md.append("\n| ç‰¹å¾1 | ç‰¹å¾2 | ç›¸å…³ç³»æ•° |")
            md.append("|-------|-------|----------|")
            for d in ca['high_correlation_pairs'][:10]:
                md.append(f"| {d['ç‰¹å¾1']} | {d['ç‰¹å¾2']} | {d['ç›¸å…³ç³»æ•°']} |")
        md.append("")
        
        # å»ºè®®
        if report['recommendations']:
            md.append("## 6. å¤„ç†å»ºè®®")
            for rec in report['recommendations']:
                md.append(f"- {rec}")
        md.append("")
        
        # å¤„ç†æ—¥å¿—
        if report['processing_log']:
            md.append("## 7. å¤„ç†æ—¥å¿—")
            for log in report['processing_log']:
                md.append(f"- {log}")
        
        return "\n".join(md)
    
    def _generate_llm_prompt(self, report: dict) -> str:
        """ç”Ÿæˆç”¨äº LLM åˆ†æçš„æç¤ºè¯"""
        prompt = []
        prompt.append("è¯·åˆ†æä»¥ä¸‹æ•°æ®é›†çš„ç‰¹å¾ï¼Œå¹¶ç»™å‡ºä¸“ä¸šçš„æ•°æ®å¤„ç†å’Œå»ºæ¨¡å»ºè®®ï¼š\n")
        
        # æ•°æ®æ¦‚è§ˆ
        ov = report['overview']
        prompt.append(f"ã€æ•°æ®è§„æ¨¡ã€‘{ov['total_rows']} è¡Œ Ã— {ov['total_columns']} åˆ—")
        prompt.append(f"ã€åˆ—ç±»å‹ã€‘æ•°å€¼åˆ— {ov['numeric_columns']} ä¸ªï¼Œåˆ†ç±»åˆ— {ov['categorical_columns']} ä¸ª")
        prompt.append(f"ã€åˆ—åã€‘{', '.join(ov['column_names'][:20])}{'...' if len(ov['column_names']) > 20 else ''}\n")
        
        # æ•°æ®è´¨é‡
        ma = report['missing_analysis']
        prompt.append(f"ã€ç¼ºå¤±æƒ…å†µã€‘æ•´ä½“ç¼ºå¤±ç‡ {ma['missing_rate']}%ï¼Œ{ma['columns_with_missing']} åˆ—æœ‰ç¼ºå¤±")
        
        oa = report['outlier_analysis']
        prompt.append(f"ã€å¼‚å¸¸å€¼ã€‘{oa['columns_with_outliers']} åˆ—æ£€æµ‹åˆ°å¼‚å¸¸å€¼")
        
        # åˆ†å¸ƒç‰¹å¾
        da = report['distribution_analysis']
        skewed = [d['column'] for d in da['details'] if abs(d.get('skewness', 0)) > 1]
        if skewed:
            prompt.append(f"ã€åæ–œåˆ†å¸ƒã€‘{', '.join(skewed)}")
        
        # ç›¸å…³æ€§
        ca = report['correlation_analysis']
        if ca['high_correlation_pairs']:
            high_corr = [f"{d['ç‰¹å¾1']}-{d['ç‰¹å¾2']}({d['ç›¸å…³ç³»æ•°']})" for d in ca['high_correlation_pairs'][:5]]
            prompt.append(f"ã€é«˜ç›¸å…³ç‰¹å¾å¯¹ã€‘{'; '.join(high_corr)}")
        
        prompt.append("\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼š")
        prompt.append("1. è¯„ä¼°æ•°æ®è´¨é‡å’Œæ½œåœ¨é—®é¢˜")
        prompt.append("2. æ¨èæ•°æ®é¢„å¤„ç†æ­¥éª¤")
        prompt.append("3. å»ºè®®é€‚åˆçš„æœºå™¨å­¦ä¹ æ¨¡å‹")
        prompt.append("4. æä¾›ç‰¹å¾å·¥ç¨‹å»ºè®®")
        
        return "\n".join(prompt)


# ==================== ä¾¿æ·å‡½æ•° ====================

def load_and_process(filepath: str, 
                     fill_missing: str = 'auto',
                     handle_outliers: str = 'cap') -> Tuple[pd.DataFrame, DataProcessor]:
    """
    ä¸€é”®åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ csv, xlsxï¼‰
        fill_missing: ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
        handle_outliers: å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥
        
    Returns:
        (å¤„ç†åçš„DataFrame, DataProcessorå¯¹è±¡)
    """
    # åŠ è½½æ•°æ®
    if filepath.endswith('.csv'):
        df = pd.read_csv(filepath)
    elif filepath.endswith('.xlsx'):
        df = pd.read_excel(filepath)
    else:
        raise ValueError("ä»…æ”¯æŒ csv å’Œ xlsx æ ¼å¼")
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶å¤„ç†
    processor = DataProcessor(df)
    processor.fill_missing(strategy=fill_missing)
    processor.handle_outliers(method=handle_outliers)
    
    return processor.get_data(), processor

